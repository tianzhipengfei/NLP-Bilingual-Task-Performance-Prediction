{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MF():\n",
    "\n",
    "    def __init__(self, R, K, alpha, beta, iterations, test_samples, X=None, Y=None, Z=None, src_si_len=0, \\\n",
    "                 tgt_si_len=0, lang_pair_si_len=0, src_index=None, tgt_index=None, model=None, num_running=0):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty\n",
    "        entries in a matrix.\n",
    "\n",
    "        Arguments\n",
    "        - R (ndarray)                  : src-tgt language rating matrix\n",
    "        - K (int)                      : number of latent dimensions\n",
    "        - alpha (float)                : learning rate\n",
    "        - beta (float)                 : regularization parameter\n",
    "        - X (dict)                     : source language side information\n",
    "        - Y (dict)                     : target language side information\n",
    "        - Z (dict)                     : language pair side information\n",
    "        - src_si_len(int)              : source language side information length\n",
    "        - tgt_si_len(int)              : target language side information length\n",
    "        - lang_pair_si_len(int)        : language pair side information length\n",
    "        \"\"\"\n",
    "\n",
    "        self.R = np.array(R)\n",
    "        self.Prediction = deepcopy(self.R)\n",
    "        self.src_langs = R.index.tolist()\n",
    "        self.tgt_langs = R.columns.tolist()\n",
    "        self.num_src, self.num_tgt = R.shape\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.Z = Z\n",
    "        self.src_si_len = src_si_len\n",
    "        self.tgt_si_len = tgt_si_len\n",
    "        self.lang_pair_si_len = lang_pair_si_len\n",
    "        self.test_samples = test_samples\n",
    "        self.src_index=src_index \n",
    "        self.tgt_index=tgt_index\n",
    "        self.model=model\n",
    "        self.num_running = num_running\n",
    "        self.score_dict = {'BLEU': \"WIKI-MT\", \"Muse\": \"BLI-Muse\", \"Vecmap\": \"BLI-Vecmap\"}\n",
    "        self.traing_error_log=[]\n",
    "        self.test_error_log=[]\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_src, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_tgt, self.K))\n",
    "\n",
    "        # Initialize side information's parameter if necesary\n",
    "        if self.X and self.src_si_len:\n",
    "            self.A = np.random.normal(scale = 1./self.src_si_len, size=self.src_si_len)\n",
    "        if self.Y and self.tgt_si_len:\n",
    "            self.B = np.random.normal(scale = 1./self.tgt_si_len, size=self.tgt_si_len)\n",
    "        if self.Z and self.lang_pair_si_len:\n",
    "            self.C = np.random.normal(scale = 1./self.lang_pair_si_len, size=self.lang_pair_si_len)\n",
    "        \n",
    "        # Initialize the biases\n",
    "        # the biases of users and items are initilized as 0\n",
    "        # the bias of rating is initilized as mean value\n",
    "        self.b_u = np.zeros(self.num_src)\n",
    "        self.b_i = np.zeros(self.num_tgt)\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "\n",
    "        # Create a list of training samples (where rating > 0)\n",
    "        self.samples = []\n",
    "        for i in range(self.num_src):\n",
    "            for j in range(self.num_tgt):\n",
    "                if self.R[i,j] > 0:\n",
    "                    cur_tuple = [i, j, self.R[i, j]]\n",
    "                    src_lang = self.src_langs[i]\n",
    "                    tgt_lang = self.tgt_langs[j]\n",
    "                    if self.X:\n",
    "                        if src_lang in self.X.keys():\n",
    "                            cur_tuple.append(self.X[src_lang])\n",
    "                        else:\n",
    "                            raise KeyError\n",
    "                    if self.Y:\n",
    "                        if tgt_lang in self.Y.keys():\n",
    "                            cur_tuple.append(self.Y[src_lang])\n",
    "                        else:\n",
    "                            raise KeyError\n",
    "                    if self.Z:\n",
    "                        if src_lang + \"_\" + tgt_lang in self.Z.keys():\n",
    "                            cur_tuple.append(self.Z[src_lang + \"_\" + tgt_lang])\n",
    "                        else:\n",
    "                            raise KeyError\n",
    "                    self.samples.append(tuple(cur_tuple))\n",
    "\n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            # shuffle training samples\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            mse = self.mse()\n",
    "            training_process.append((i, mse))\n",
    "            if (i+1) % 100 == 0:\n",
    "                self.traing_error_log.append((i, mse))\n",
    "                test_mse = self.evaluate_testing(self.test_samples, self.src_index, self.tgt_index, self.model)\n",
    "                self.test_error_log.append((i, test_mse))\n",
    "#                 print(\"\\t\\tIteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "\n",
    "        return training_process\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "#         predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x][y] - self.Prediction[x][y], 2)\n",
    "        return np.sqrt(error / len(xs))\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for sample in self.samples:\n",
    "            i, j, r = sample[0], sample[1], sample[2]\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(sample)\n",
    "            self.Prediction[i][j] = prediction\n",
    "            e = (r - prediction)\n",
    "\n",
    "            # Update biases\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])\n",
    "\n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * self.P[i, :] - self.beta * self.Q[j,:])\n",
    "            \n",
    "            # Update side information parameter if necessary\n",
    "            cur_index = 3\n",
    "            if self.X:\n",
    "                x = np.array(sample[cur_index], dtype=np.float64)\n",
    "                cur_index += 1\n",
    "                self.A += self.alpha * (e * x - self.beta * self.A)\n",
    "            if self.Y:\n",
    "                y = np.array(sample[cur_index], dtype=np.float64)\n",
    "                cur_index += 1\n",
    "                self.B += self.alpha * (e * y - self.beta * self.B)\n",
    "            if self.Z:\n",
    "                z = np.array(sample[cur_index], dtype=np.float64)\n",
    "                self.C += self.alpha * (e * z - self.beta * self.C)\n",
    "                \n",
    "\n",
    "    def get_rating(self, sample):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of sample\n",
    "        \"\"\"\n",
    "        i, j, r = sample[0], sample[1], sample[2]\n",
    "        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        cur_index = 3\n",
    "        if self.X:\n",
    "            x = sample[cur_index]\n",
    "            cur_index += 1\n",
    "            prediction += self.A.dot(x.T)\n",
    "        if self.Y:\n",
    "            y = sample[cur_index]\n",
    "            cur_index += 1\n",
    "            prediction += self.B.dot(y.T)\n",
    "        if self.Z:\n",
    "            z = sample[cur_index]\n",
    "            prediction += self.C.dot(z.T)\n",
    "        return prediction\n",
    "    \n",
    "    def evaluate_testing(self, test_data, src_index_name, tgt_index_name, score_index_name):\n",
    "        \"\"\"\n",
    "        Predict the score for testing data\n",
    "        \"\"\" \n",
    "        rmse = 0.0\n",
    "        for record in test_data.iterrows():\n",
    "            record = record[1]\n",
    "            src_lang = record[src_index_name]\n",
    "            tgt_lang = record[tgt_index_name]\n",
    "            src_lang_index = self.src_langs.index(src_lang)\n",
    "            tgt_lang_index = self.tgt_langs.index(tgt_lang)\n",
    "            score = record[score_index_name]\n",
    "            cur_tuple = [src_lang_index, tgt_lang_index, score]\n",
    "            if self.X:\n",
    "                if src_lang in self.X.keys():\n",
    "                    cur_tuple.append(self.X[src_lang])\n",
    "                else:\n",
    "                    raise KeyError\n",
    "            if self.Y:\n",
    "                if tgt_lang in self.Y.keys():\n",
    "                    cur_tuple.append(self.Y[src_lang])\n",
    "                else:\n",
    "                    raise KeyError\n",
    "            if self.Z:\n",
    "                if src_lang + \"_\" + tgt_lang in self.Z.keys():\n",
    "                    cur_tuple.append(self.Z[src_lang + \"_\" + tgt_lang])\n",
    "                else:\n",
    "                    raise KeyError\n",
    "            prediction = self.get_rating(tuple(cur_tuple))\n",
    "            rmse += (prediction - score) * (prediction - score)\n",
    "        return np.sqrt(rmse / len(test_data))\n",
    "        \n",
    "\n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P, Q, A, B, and C\n",
    "        \"\"\"\n",
    "        res = deepcopy(self.R)\n",
    "        for i in range(self.num_src):\n",
    "            for j in range(self.num_tgt):\n",
    "                src_lang = self.src_langs[i]\n",
    "                tgt_lang = self.tgt_langs[j]\n",
    "                res[i][j] = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "                if self.X and src_lang in self.X.keys():\n",
    "                    x = self.X[src_lang]\n",
    "                    res[i][j] += self.A.dot(x.T)\n",
    "                if self.Y and tgt_lang in self.Y.keys():\n",
    "                    y = self.X[tgt_lang]\n",
    "                    res[i][j] += self.A.dot(x.T)\n",
    "                if self.Z and src_lang + \"_\" + tgt_lang in self.Z.keys():\n",
    "                    z = self.X[src_lang + \"_\" + tgt_lang]\n",
    "                    res[i][j] += self.A.dot(x.T)\n",
    "        return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T)\n",
    "    \n",
    "    def draw_error_curve(self, i):\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        iters = []\n",
    "        train_loss = []\n",
    "        test_loss = []\n",
    "        for item in self.traing_error_log:\n",
    "            iters.append(item[0])\n",
    "            train_loss.append(item[1])\n",
    "        for item in self.test_error_log:\n",
    "            test_loss.append(item[1])\n",
    "\n",
    "        plt.plot(iters, train_loss, 'b', label='train loss')#'b'指：color='blue'\n",
    "        plt.plot(iters, test_loss, 'r', label='test loss')#'r'指：color='red'\n",
    "\n",
    "        plt.legend()  #显示上面的label\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        title = self.score_dict[self.model] + \"_\" + str(self.num_running+1) + \"_Fold\" + str(i+1)\n",
    "        plt.title(title) \n",
    "        plt.savefig(\"../result/within_pair_si/\" + self.score_dict[self.model] + \"/fold\" + str(i+1) + \"/\" + title + \".png\")\n",
    "\n",
    "        #plt.ylim(-1,1)#仅设置y轴坐标范围\n",
    "        plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represent the two tasks above as matrices (BLEU scores for Wiki-MT and Accuracy for BLI) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "\n",
    "def split_k_fold_data(file_dir, score_index_name, src_index_name, tgt_index_name, k=5):\n",
    "    data = pd.read_csv(file_dir)\n",
    "    \n",
    "    # shuffle\n",
    "    data = data.sample(frac=1)\n",
    "    \n",
    "    # generate score matrix\n",
    "    src_langs = data[src_index_name].unique()\n",
    "    tgt_langs = data[tgt_index_name].unique()\n",
    "    score_matrix = pd.DataFrame(index = src_langs, columns = tgt_langs)\n",
    "    \n",
    "    # eliminate empty rows and columns\n",
    "    data = data.dropna(axis=1, how=\"all\")\n",
    "    data = data.dropna(axis=0, how=\"all\")\n",
    "    \n",
    "    # K fold split\n",
    "    k_fold_data = {}\n",
    "    models = list(score_index_name)\n",
    "    lens= len(data)\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        ex_per_fold = int(np.ceil(lens / k))\n",
    "        for j in range(k):\n",
    "            start = ex_per_fold * j\n",
    "            end = ex_per_fold * (j + 1)\n",
    "            if j == 0:\n",
    "                k_fold_data[model] = {\"train\": [], \"test\": []}\n",
    "            k_fold_data[model][\"train\"].append(pd.concat([data.iloc[:start, :], data.iloc[end:, :]], axis=0))\n",
    "            k_fold_data[model][\"test\"].append(data.iloc[start:end, :])\n",
    "    return k_fold_data, score_matrix\n",
    "\n",
    "def generate_score_matrix(train_data, src_index_name, tgt_index_name, score_index_name, origin_score_matrix):    \n",
    "    score_matrix = deepcopy(origin_score_matrix)\n",
    "    \n",
    "    for record in train_data.iterrows():\n",
    "        record = record[1]\n",
    "        src_lang = record[src_index_name]\n",
    "        tgt_lang = record[tgt_index_name]\n",
    "        score = record[score_index_name]\n",
    "        score_matrix.loc[src_lang, tgt_lang] = score\n",
    "#         score_matrix[src_lang][tgt_lang] = score\n",
    "    np.fill_diagonal(score_matrix, 100.0)\n",
    "    score_matrix.fillna(0, inplace=True)\n",
    "    return score_matrix\n",
    "\n",
    "def get_rmse(valid_data, model, src_index_name, tgt_index_name, score_matrix, train_matrix):\n",
    "    rmse = 0.0\n",
    "    src_langs = train_matrix.index.tolist()\n",
    "    tgt_langs = train_matrix.columns.tolist()\n",
    "    for cur_valid_data in valid_data.iterrows():\n",
    "        cur_valid_data = cur_valid_data[1]\n",
    "        src_lang, tgt_lang, score = cur_valid_data[src_index_name], cur_valid_data[tgt_index_name], cur_valid_data[model]\n",
    "        src_idx = src_langs.index(src_lang)\n",
    "        tgt_idx = tgt_langs.index(tgt_lang)\n",
    "        prediction = score_matrix[src_idx][tgt_idx]\n",
    "        rmse += (prediction - score) * (prediction - score)\n",
    "    return np.sqrt(rmse / len(valid_data))\n",
    "\n",
    "def get_result(alpha, beta, data_dir, scores, src_index, tgt_index, k, num_running, src_lang_side_info=None, \\\n",
    "               tgt_lang_side_info=None, lang_pair_side_info=None, src_si_len=0, tgt_si_len=0, \\\n",
    "               lang_pair_si_len=0):\n",
    "    all_running_rmse = 0.0\n",
    "    for nr in range(num_running):\n",
    "        print(\"num running: \" + str(nr+1))\n",
    "        data, langs_matrix = split_k_fold_data(data_dir, scores, src_index, tgt_index, k)\n",
    "        res = {}\n",
    "        for model in scores:\n",
    "            print(\"-\"*40)\n",
    "            print(model)\n",
    "            total_rmse = 0.0\n",
    "            for i in range(k):\n",
    "                print(\"\\tFold {}: \".format(i+1))\n",
    "                train_data, test_data = data[model][\"train\"][i], data[model][\"test\"][i]\n",
    "                train_matrix = generate_score_matrix(train_data, src_index, tgt_index, model, langs_matrix)\n",
    "                mf = MF(train_matrix, K=2, alpha=alpha, beta=beta, iterations=2000, test_samples=test_data, X=src_lang_side_info, \\\n",
    "                        Y=tgt_lang_side_info, Z=lang_pair_side_info, src_si_len=src_si_len, tgt_si_len=tgt_si_len, \\\n",
    "                        lang_pair_si_len=lang_pair_si_len, src_index=src_index, tgt_index=tgt_index, model=model, \\\n",
    "                       num_running=nr)\n",
    "                trainging_log = mf.train()\n",
    "#                 predictions = mf.full_matrix()\n",
    "#                 cur_rmse = get_rmse(test_data, model, src_index, tgt_index, predictions, train_matrix)\n",
    "                cur_rmse = mf.evaluate_testing(test_data, src_index, tgt_index, model)\n",
    "                mf.draw_error_curve(i)\n",
    "                total_rmse += cur_rmse\n",
    "                print(\"\\t\\trmse is {}.\".format(cur_rmse))\n",
    "                print(\"*\" * 20)\n",
    "                \n",
    "            average_rmse = total_rmse / k\n",
    "            print(\"average rmse: \" + str(average_rmse))\n",
    "            res[model] = average_rmse\n",
    "        res_rmse = 0.0\n",
    "        for key, value in res.items():\n",
    "            res_rmse += value\n",
    "        all_running_rmse += res_rmse / len(scores)\n",
    "    return all_running_rmse / num_running\n",
    "\n",
    "def get_language_pair_side_info(data_dir, side_info_features, src_lang_name, tgt_lang_name):\n",
    "    data = pd.read_csv(data_dir)\n",
    "    side_dict = {}\n",
    "    for record in data.iterrows():\n",
    "        record = record[1]\n",
    "        src_lang = record[src_lang_name]\n",
    "        tgt_lang = record[tgt_lang_name]\n",
    "        side_dict[src_lang + \"_\" + tgt_lang] = record[side_info_features].values\n",
    "    return side_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num running: 1\n",
      "----------------------------------------\n",
      "BLEU\n",
      "\tFold 1: \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'flat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b0794917a3d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m wiki_result = get_result(WIKI_ALPHA, WIKI_BETA, WIKI_MT_DIR, WIKI_SCORE, WIKI_SRC, WIKI_TGT, k, num_running, \\\n\u001b[0;32m---> 25\u001b[0;31m                          lang_pair_side_info = WIKI_SIDE_INFO_DICT, lang_pair_si_len = 6)\n\u001b[0m\u001b[1;32m     26\u001b[0m bli_result = get_result(BLI_ALPHA, BLI_BETA, BLI_DIR, BLI_SCORE, BLI_SRC, BLI_TGT, k, num_running, \\\n\u001b[1;32m     27\u001b[0m                          lang_pair_side_info = BLI_SIDE_INFO_DICT, lang_pair_si_len = 6)\n",
      "\u001b[0;32m<ipython-input-4-4ae7f9ec17fe>\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(alpha, beta, data_dir, scores, src_index, tgt_index, k, num_running, src_lang_side_info, tgt_lang_side_info, lang_pair_side_info, src_si_len, tgt_si_len, lang_pair_si_len)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\tFold {}: \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mtrain_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_score_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangs_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                 mf = MF(train_matrix, K=2, alpha=alpha, beta=beta, iterations=2000, test_samples=test_data, X=src_lang_side_info, \\\n\u001b[1;32m     80\u001b[0m                         \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_lang_side_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang_pair_side_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_si_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_si_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_si_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_si_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4ae7f9ec17fe>\u001b[0m in \u001b[0;36mgenerate_score_matrix\u001b[0;34m(train_data, src_index_name, tgt_index_name, score_index_name, origin_score_matrix)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mscore_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msrc_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_lang\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#         score_matrix[src_lang][tgt_lang] = score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_diagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mscore_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscore_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mfill_diagonal\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/lib/index_tricks.py\u001b[0m in \u001b[0;36mfill_diagonal\u001b[0;34m(a, val, wrap)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m     \u001b[0;31m# Write the value out into the diagonal.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5179\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5180\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'flat'"
     ]
    }
   ],
   "source": [
    "WIKI_MT_DIR = \"../data/data_wiki.csv\"\n",
    "WIKI_SRC = \"Source\"\n",
    "WIKI_TGT = \"Target\"\n",
    "WIKI_SCORE = [\"BLEU\"]\n",
    "WIKI_SIDE_FEATURES = ['geographic', 'genetic', 'inventory', 'syntactic', 'phonological', 'featural']\n",
    "WIKI_ALPHA = 0.0001\n",
    "WIKI_BETA = 0.0005\n",
    "\n",
    "BLI_DIR = \"../data/data_bli2.csv\"\n",
    "BLI_SRC = \"Source Language Code\"\n",
    "BLI_TGT = \"Target Language Code\"\n",
    "BLI_SCORE = [\"Muse\", \"Vecmap\"]\n",
    "BLI_SIDE_FEATURES = ['genetic', 'syntactic', 'featural', 'phonological', 'inventory', 'geographic']\n",
    "BLI_ALPHA = 0.0001\n",
    "BLI_BETA = 0.0005\n",
    "# The score metric is from \"NLPerf/src/task_feats.py\"\n",
    "\n",
    "k = 5\n",
    "num_running = 10\n",
    "\n",
    "WIKI_SIDE_INFO_DICT = get_language_pair_side_info(WIKI_MT_DIR, WIKI_SIDE_FEATURES, WIKI_SRC, WIKI_TGT)\n",
    "BLI_SIDE_INFO_DICT = get_language_pair_side_info(BLI_DIR, BLI_SIDE_FEATURES, BLI_SRC, BLI_TGT)\n",
    "\n",
    "wiki_result = get_result(WIKI_ALPHA, WIKI_BETA, WIKI_MT_DIR, WIKI_SCORE, WIKI_SRC, WIKI_TGT, k, num_running, \\\n",
    "                         lang_pair_side_info = WIKI_SIDE_INFO_DICT, lang_pair_si_len = 6)\n",
    "bli_result = get_result(BLI_ALPHA, BLI_BETA, BLI_DIR, BLI_SCORE, BLI_SRC, BLI_TGT, k, num_running, \\\n",
    "                         lang_pair_side_info = BLI_SIDE_INFO_DICT, lang_pair_si_len = 6)\n",
    "\n",
    "# wiki_mt_score_matrix = generate_score_matrix(WIKI_MT_DIR, \"Source\", \"Target\", \"BLEU\")\n",
    "# wiki_mt_score_matrix, wiki_mt_valid = generate_score_matrix(WIKI_MT_DIR, \"Source\", \"Target\", \"BLEU\")xdz\n",
    "# bli_score_matrix, bli_valid = generate_score_matrix(BLI_DIR, \"Source Language Code\", \"Target Language Code\", [\"Muse\", \"Vecmap\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wiki_result)\n",
    "print(bli_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
